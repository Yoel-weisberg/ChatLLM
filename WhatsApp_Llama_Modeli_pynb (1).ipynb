{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G0sxPIXkuXFk"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing Packages"
      ],
      "metadata": {
        "id": "G0sxPIXkuXFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets pandas numpy peft accelerate"
      ],
      "metadata": {
        "id": "FO50P0VHucWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdeeuTYGxvQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting google drive"
      ],
      "metadata": {
        "id": "T9taIwsLqA23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MQU5SEV0efdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your WhatsApp chat file in Google Drive\n",
        "chat_file_path = \"/content/drive/MyDrive/Data-ChatLLM/input.txt\"  # Update this path\n",
        "\n",
        "# Verify the file exists\n",
        "import os\n",
        "if os.path.exists(chat_file_path):\n",
        "    print(f\"File found: {chat_file_path}\")\n",
        "    # Show first few lines to verify content\n",
        "    with open(chat_file_path, 'r', encoding='utf-8') as f:\n",
        "        print(\"First 5 lines of the file:\")\n",
        "        for i, line in enumerate(f):\n",
        "            if i < 5:\n",
        "                print(line.strip())\n",
        "            else:\n",
        "                break\n",
        "else:\n",
        "    print(f\"File not found: {chat_file_path}\")\n",
        "    print(\"Available files in MyDrive:\")\n",
        "    !ls \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "-OHfRwEPxwW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### importing"
      ],
      "metadata": {
        "id": "D1OEnNuSV74s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, \\\n",
        "    DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "import random"
      ],
      "metadata": {
        "id": "VAJ1lNeiV7FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The fine tuning itself\n",
        "(can be skiped - if already in drive)"
      ],
      "metadata": {
        "id": "ETPMjx3-ZTKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### parsing"
      ],
      "metadata": {
        "id": "QDk5l7EtWA2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Parse the WhatsApp chat data\n",
        "def parse_whatsapp_chat(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parse WhatsApp chat export file into a structured DataFrame using string separators.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the WhatsApp chat export file\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with columns: timestamp, sender, message\n",
        "    \"\"\"\n",
        "    # Lists to store extracted data\n",
        "    timestamps = []\n",
        "    senders = []\n",
        "    messages = []\n",
        "\n",
        "    # Read the file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Process each line\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Check if this line starts a new message\n",
        "        # Format: 10/29/23, 9:28 AM - Yoel Weisberg: זה לדבר על הערוץ\n",
        "        if \" - \" in line and \": \" in line and len(\n",
        "                line.split(\" - \")[0].strip()) >= 10:  # Timestamp usually at least 10 chars\n",
        "            # Split by the first occurrence of \" - \"\n",
        "            parts = line.split(\" - \", 1)\n",
        "\n",
        "            if len(parts) == 2:\n",
        "                timestamp = parts[0].strip()\n",
        "\n",
        "                # Split the second part by the first occurrence of \": \"\n",
        "                sender_message_parts = parts[1].split(\": \", 1)\n",
        "\n",
        "                if len(sender_message_parts) == 2:\n",
        "                    sender = sender_message_parts[0].strip()\n",
        "                    message = sender_message_parts[1].strip()\n",
        "\n",
        "                    timestamps.append(timestamp)\n",
        "                    senders.append(sender)\n",
        "                    messages.append(message)\n",
        "                else:\n",
        "                    # This might be a system message or special format\n",
        "                    continue\n",
        "        else:\n",
        "            # If it doesn't match the expected format, it might be a continuation of the previous message\n",
        "            if messages and senders and timestamps:\n",
        "                messages[-1] += \" \" + line\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'timestamp': timestamps,\n",
        "        'sender': senders,\n",
        "        'message': messages\n",
        "    })\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "S4QACXgmWJJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### prapering data for training"
      ],
      "metadata": {
        "id": "eTvId9UNWNaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare data for training\n",
        "def prepare_data_for_training(df: pd.DataFrame, min_messages: int = 20) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare chat data for training by organizing messages by sender.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing parsed WhatsApp chat\n",
        "        min_messages: Minimum number of messages required for a sender to be included\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping sender names to their messages\n",
        "    \"\"\"\n",
        "    # Count messages per sender\n",
        "    sender_counts = df['sender'].value_counts()\n",
        "\n",
        "    # Filter senders with enough messages\n",
        "    valid_senders = sender_counts[sender_counts >= min_messages].index.tolist()\n",
        "\n",
        "    # Create dictionary of sender -> messages\n",
        "    sender_messages = {}\n",
        "    for sender in valid_senders:\n",
        "        sender_messages[sender] = df[df['sender'] == sender]['message'].tolist()\n",
        "\n",
        "    return sender_messages"
      ],
      "metadata": {
        "id": "DZJ7hChvWQc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### creating dataset"
      ],
      "metadata": {
        "id": "wSa5nrkxWVar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create training data for Llama fine-tuning\n",
        "def create_training_dataset(sender_messages: Dict[str, List[str]],\n",
        "                            context_length: int = 3,\n",
        "                            max_length: int = 512) -> Dataset:\n",
        "    \"\"\"\n",
        "    Create a dataset for fine-tuning Llama model.\n",
        "\n",
        "    Args:\n",
        "        sender_messages: Dictionary mapping sender names to their messages\n",
        "        context_length: Number of previous messages to use as context\n",
        "        max_length: Maximum length of input sequences\n",
        "\n",
        "    Returns:\n",
        "        Dataset suitable for fine-tuning\n",
        "    \"\"\"\n",
        "    training_examples = []\n",
        "\n",
        "    for sender, messages in sender_messages.items():\n",
        "        for i in range(context_length, len(messages)):\n",
        "            # Create context from previous messages\n",
        "            context = messages[i - context_length:i]\n",
        "            context_text = \" \".join([f\"Message: {msg}\" for msg in context])\n",
        "\n",
        "            # Create input with format that tells the model who should respond\n",
        "            input_text = f\"Context: {context_text}\\nGenerate {sender}'s response:\"\n",
        "\n",
        "            # Target is the actual response\n",
        "            target_text = messages[i]\n",
        "\n",
        "            training_examples.append({\n",
        "                \"input\": input_text,\n",
        "                \"target\": target_text,\n",
        "                \"sender\": sender\n",
        "            })\n",
        "\n",
        "    # Convert to Dataset\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"input\": [example[\"input\"] for example in training_examples],\n",
        "        \"target\": [example[\"target\"] for example in training_examples],\n",
        "        \"sender\": [example[\"sender\"] for example in training_examples]\n",
        "    })\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "OhUpmexoWX3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenising"
      ],
      "metadata": {
        "id": "9yvnkPLvXpvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Tokenization functions\n",
        "def tokenize_function(examples, tokenizer, max_length):\n",
        "    \"\"\"Tokenize the input and target texts.\"\"\"\n",
        "    # Tokenize inputs\n",
        "    inputs = tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "    # Tokenize targets with special handling for the EOS token\n",
        "    targets = tokenizer(examples[\"target\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "    # Prepare inputs and labels for training\n",
        "    examples[\"input_ids\"] = inputs[\"input_ids\"]\n",
        "    examples[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
        "    examples[\"labels\"] = targets[\"input_ids\"]\n",
        "\n",
        "    return examples"
      ],
      "metadata": {
        "id": "Eb3uTOzdXsfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### fine tuning"
      ],
      "metadata": {
        "id": "18Eka9AuXw0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Fine-tune the Llama model\n",
        "def fine_tune_llama(dataset, model_name=\"meta-llama/Llama-2-7b-hf\", output_dir=\"./fine_tuned_llama\", max_length=512,\n",
        "                    use_peft=True, batch_size=1, fp16=True):\n",
        "    \"\"\"\n",
        "    Fine-tune a Llama model on WhatsApp chat data.\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset containing input-target pairs\n",
        "        model_name: HuggingFace model name/path\n",
        "        output_dir: Directory to save the fine-tuned model\n",
        "        max_length: Maximum sequence length\n",
        "        use_peft: Whether to use PEFT/LoRA for more efficient fine-tuning\n",
        "        batch_size: Batch size for training\n",
        "        fp16: Whether to use mixed precision training\n",
        "\n",
        "    Returns:\n",
        "        Fine-tuned model and tokenizer\n",
        "    \"\"\"\n",
        "    # Try to import the necessary libraries\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "    except ImportError:\n",
        "        print(\"Warning: bitsandbytes not installed. Quantization won't be available.\")\n",
        "        BitsAndBytesConfig = None\n",
        "\n",
        "    try:\n",
        "        import accelerate\n",
        "        print(f\"Using accelerate version: {accelerate.__version__}\")\n",
        "    except ImportError:\n",
        "        print(\"Warning: accelerate not installed. Installing it now...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call([\"pip\", \"install\", \"accelerate>=0.26.0\"])\n",
        "        import accelerate\n",
        "        print(f\"Installed accelerate version: {accelerate.__version__}\")\n",
        "\n",
        "    # Set up quantization and PEFT if requested\n",
        "    model_kwargs = {}\n",
        "    peft_config = None\n",
        "\n",
        "    if use_peft:\n",
        "        try:\n",
        "            from peft import LoraConfig, get_peft_model, TaskType\n",
        "            print(\"Using PEFT/LoRA for efficient fine-tuning\")\n",
        "\n",
        "            # Define LoRA config\n",
        "            peft_config = LoraConfig(\n",
        "                task_type=TaskType.CAUSAL_LM,\n",
        "                inference_mode=False,\n",
        "                r=8,  # rank\n",
        "                lora_alpha=32,\n",
        "                lora_dropout=0.1,\n",
        "                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"Warning: peft not installed. Falling back to full fine-tuning.\")\n",
        "            print(\"To use PEFT, install it with: pip install peft\")\n",
        "            use_peft = False\n",
        "\n",
        "    # Check if GPU is available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if device == \"cpu\":\n",
        "        print(\"WARNING: Training on CPU will be very slow\")\n",
        "        fp16 = False  # Disable fp16 on CPU\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Add padding token if needed\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load model with appropriate configuration\n",
        "    if BitsAndBytesConfig and device == \"cuda\" and use_peft:\n",
        "        try:\n",
        "            # Try to use 4-bit quantization for efficiency\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "            )\n",
        "            model_kwargs[\"quantization_config\"] = quantization_config\n",
        "            print(\"Using 4-bit quantization\")\n",
        "        except Exception as e:\n",
        "            print(f\"Quantization setup failed: {e}\")\n",
        "            print(\"Falling back to standard loading\")\n",
        "\n",
        "    # Load the model\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=device if device == \"cuda\" else None,\n",
        "            **model_kwargs\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model with advanced options: {e}\")\n",
        "        print(\"Trying with basic configuration...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Apply LoRA if available and requested\n",
        "    if use_peft and peft_config:\n",
        "        try:\n",
        "            model = get_peft_model(model, peft_config)\n",
        "            print(\"LoRA applied successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying LoRA: {e}\")\n",
        "            print(\"Continuing with full fine-tuning\")\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda examples: tokenize_function(examples, tokenizer, max_length),\n",
        "        batched=True,\n",
        "        remove_columns=[\"input\", \"target\", \"sender\"]\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # We're not using masked language modeling\n",
        "    )\n",
        "\n",
        "    # Set up training arguments\n",
        "    # Set up training arguments with fixes\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=4,\n",
        "        save_steps=1000,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "        logging_dir=\"./logs\",\n",
        "        fp16=fp16 and device == \"cuda\",\n",
        "        optim=\"adamw_torch\",\n",
        "        logging_steps=100,\n",
        "        warmup_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        # Fix for gradient checkpointing\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "        label_names=[\"labels\"],\n",
        "        report_to=None\n",
        "        # Explicitly set use_cache to False\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer with label_names parameter\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=tokenized_dataset\n",
        "        # Add this line to fix the label_names warning\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "J0j0LTWXXzbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pipeline"
      ],
      "metadata": {
        "id": "kX-AIctRYFm4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYrcOI6PdWsA"
      },
      "outputs": [],
      "source": [
        "# Step 8: Complete pipeline function\n",
        "def whatsapp_llama_pipeline(chat_file_path: str, model_name: str = \"meta-llama/Llama-2-7b-hf\",\n",
        "                            output_dir: str = \"./fine_tuned_llama\", use_peft: bool = True,\n",
        "                            batch_size: int = 1, fp16: bool = True, skip_training: bool = False):\n",
        "    \"\"\"\n",
        "    Complete pipeline from WhatsApp chat data to fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        chat_file_path: Path to WhatsApp chat export file\n",
        "        model_name: Base model to fine-tune\n",
        "        output_dir: Directory to save the fine-tuned model\n",
        "        use_peft: Whether to use PEFT/LoRA for efficient fine-tuning\n",
        "        batch_size: Batch size for training\n",
        "        fp16: Whether to use mixed precision training\n",
        "        skip_training: Skip the training step (for testing the pipeline)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Step 1: Parsing WhatsApp chat data...\")\n",
        "        df = parse_whatsapp_chat(chat_file_path)\n",
        "\n",
        "        print(f\"Found {len(df)} messages from {df['sender'].nunique()} different senders\")\n",
        "\n",
        "        print(\"\\nStep 2: Preparing data for training...\")\n",
        "        sender_messages = prepare_data_for_training(df)\n",
        "        print(f\"Found {len(sender_messages)} senders with enough messages\")\n",
        "\n",
        "        if len(sender_messages) == 0:\n",
        "            print(\"No senders with enough messages found. Try reducing the min_messages parameter.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nStep 3: Creating training dataset...\")\n",
        "        dataset = create_training_dataset(sender_messages)\n",
        "        print(f\"Created dataset with {len(dataset)} training examples\")\n",
        "\n",
        "        if not skip_training:\n",
        "            print(\"\\nStep 4: Fine-tuning Llama model...\")\n",
        "            print(f\"Using base model: {model_name}\")\n",
        "            print(f\"This may take a while depending on your hardware...\")\n",
        "\n",
        "            # Check system resources\n",
        "            import os\n",
        "            import psutil\n",
        "\n",
        "            try:\n",
        "                process = psutil.Process(os.getpid())\n",
        "                memory_gb = process.memory_info().rss / (1024 * 1024 * 1024)\n",
        "                print(f\"Current process memory usage: {memory_gb:.2f} GB\")\n",
        "\n",
        "                total_memory = psutil.virtual_memory().total / (1024 * 1024 * 1024)\n",
        "                print(f\"Total system memory: {total_memory:.2f} GB\")\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    for i in range(torch.cuda.device_count()):\n",
        "                        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024 * 1024 * 1024)\n",
        "                        print(f\"GPU {i} total memory: {gpu_memory:.2f} GB\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not check system resources: {e}\")\n",
        "\n",
        "            model, tokenizer = fine_tune_llama(\n",
        "                dataset,\n",
        "                model_name=model_name,\n",
        "                output_dir=output_dir,\n",
        "                use_peft=use_peft,\n",
        "                batch_size=batch_size,\n",
        "                fp16=fp16\n",
        "            )\n",
        "\n",
        "            print(f\"\\nModel successfully fine-tuned and saved to {output_dir}\")\n",
        "        else:\n",
        "            print(\"\\nSkipping training step as requested.\")\n",
        "            print(\"Loading tokenizer only for chat interface...\")\n",
        "            from transformers import AutoTokenizer\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"Error in pipeline: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Running the *pipeline*"
      ],
      "metadata": {
        "id": "LS_YdB-td8OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whatsapp_llama_pipeline(\n",
        "    chat_file_path=chat_file_path,\n",
        "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # Smaller model for faster training\n",
        "    output_dir=\"/content/drive/MyDrive/Data-ChatLLM/whatsapp_model\",  # Save to Drive\n",
        "    use_peft=True,  # Use PEFT/LoRA for efficient training\n",
        "    batch_size=1,\n",
        "    fp16=True  # Use mixed precision (works well with Colab GPUs)\n",
        ")"
      ],
      "metadata": {
        "id": "yMMs3Oor1POy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the model from drive\n",
        "(go here if alreday in drive)"
      ],
      "metadata": {
        "id": "sflm182CGnSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated working code\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Check what's in the directory first\n",
        "checkpoint_path = \"/content/drive/MyDrive/Data-ChatLLM/whatsapp_model/checkpoint-4000\"\n",
        "print(\"Files in checkpoint directory:\")\n",
        "print(os.listdir(checkpoint_path))\n",
        "\n",
        "# Load the model and tokenizer\n",
        "try:\n",
        "    # First try loading from the checkpoint\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        checkpoint_path,\n",
        "        use_fast=True\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        checkpoint_path,\n",
        "        device_map=\"auto\"  # This handles GPU placement automatically\n",
        "    )\n",
        "\n",
        "    print(\"Successfully loaded from checkpoint!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading from checkpoint: {e}\")\n",
        "    print(\"\\nTrying to load from the main model directory instead...\")\n",
        "\n",
        "    # If that fails, try the main model directory\n",
        "    model_path = \"/content/drive/MyDrive/whatsapp_model\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "    print(\"Successfully loaded from main model directory!\")\n"
      ],
      "metadata": {
        "id": "Y5aKKAG2Gp7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### interface function"
      ],
      "metadata": {
        "id": "Dfl5hRVvY5RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(input_message, friend_name, max_length=100, context=None, temperature=0.2):\n",
        "    # Add context from previous messages if available\n",
        "    if context is None:\n",
        "        context = []\n",
        "\n",
        "    # Create a more structured prompt with context\n",
        "    if context:\n",
        "        context_str = \"\\n\".join([f\"Previous message: {msg}\" for msg in context])\n",
        "        prompt = f\"{context_str}\\nMessage: {input_message}\\nGenerate {friend_name}'s response:\"\n",
        "    else:\n",
        "        prompt = f\"Message: {input_message}\\nGenerate {friend_name}'s response:\"\n",
        "\n",
        "    # Tokenize input with explicit attention mask\n",
        "    encoded_input = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    # Make sure attention mask is properly set\n",
        "    if 'attention_mask' not in encoded_input:\n",
        "        # Create attention mask manually\n",
        "        input_ids = encoded_input['input_ids']\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).long() if tokenizer.pad_token_id is not None else torch.ones_like(input_ids)\n",
        "        encoded_input['attention_mask'] = attention_mask\n",
        "\n",
        "    # Move everything to the correct device\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}\n",
        "\n",
        "    # Generate response with more control parameters\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            encoded_input['input_ids'],\n",
        "            attention_mask=encoded_input['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            max_new_tokens=max_length,\n",
        "            min_length=5,  # Avoid extremely short responses\n",
        "            temperature=temperature,  # Control randomness\n",
        "            top_p=0.92,  # Slightly increased for more diversity\n",
        "            top_k=50,  # Limit vocabulary to top 50 choices at each step\n",
        "            do_sample=True,\n",
        "            no_repeat_ngram_size=3,  # Avoid repeating the same phrases\n",
        "            num_return_sequences=1,  # Generate one sequence\n",
        "            pad_token_id=tokenizer.eos_token_id  # Proper padding\n",
        "        )\n",
        "\n",
        "    # Decode the response\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the response part with improved handling\n",
        "    if \"Generate\" in generated_text and \"response:\" in generated_text:\n",
        "        response = generated_text.split(\"response:\")[-1].strip()\n",
        "    elif input_message in generated_text:\n",
        "        response = generated_text.split(input_message)[-1].strip()\n",
        "    else:\n",
        "        # If both methods fail, try to find a sensible section\n",
        "        response = generated_text.strip()\n",
        "        # Remove the prompt part if it's included\n",
        "        prompt_parts = [f\"Generate {friend_name}'s response\", input_message]\n",
        "        for part in prompt_parts:\n",
        "            if part in response:\n",
        "                response = response.split(part)[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Improved interface with context tracking\n",
        "def test_model():\n",
        "    print(\"=== WhatsApp Friend Simulator (Enhanced Version) ===\")\n",
        "    print(\"Enter a friend's name or type 'quit' to exit\")\n",
        "    print(\"Type 'temp' followed by a number (0.1-1.5) to adjust creativity\")\n",
        "\n",
        "    context = []  # Keep track of conversation history\n",
        "    current_friend = None\n",
        "\n",
        "    while True:\n",
        "        if current_friend is None:\n",
        "            friend_name = input(\"\\nFriend name: \")\n",
        "            if friend_name.lower() == 'quit':\n",
        "                break\n",
        "            current_friend = friend_name\n",
        "\n",
        "        message = input(\"Your message (or 'switch' to change friend, 'clear' for new convo): \")\n",
        "\n",
        "        if message.lower() == 'quit':\n",
        "            break\n",
        "        elif message.lower() == 'switch':\n",
        "            current_friend = None\n",
        "            continue\n",
        "        elif message.lower() == 'clear':\n",
        "            context = []\n",
        "            print(\"Conversation history cleared.\")\n",
        "            continue\n",
        "        elif message.lower().startswith('temp '):\n",
        "            try:\n",
        "                new_temp = float(message.split(' ')[1])\n",
        "                if 0.1 <= new_temp <= 1.5:\n",
        "                    temperature = new_temp\n",
        "                    print(f\"Temperature set to {temperature} (higher = more creative, lower = more consistent)\")\n",
        "                else:\n",
        "                    print(\"Temperature must be between 0.1 and 1.5\")\n",
        "            except:\n",
        "                print(\"Invalid temperature format. Use 'temp 0.9' for example.\")\n",
        "            continue\n",
        "\n",
        "        response = generate_response(\n",
        "            message,\n",
        "            current_friend,\n",
        "            max_length=100,  # Longer responses\n",
        "            context=context[-3:] if context else None,  # Use last 3 messages\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{current_friend}: {response}\")\n",
        "\n",
        "        # Update context\n",
        "        context.append(message)\n",
        "        context.append(response)\n",
        "\n",
        "        # Keep context manageable\n",
        "        if len(context) > 10:\n",
        "            context = context[-10:]"
      ],
      "metadata": {
        "id": "mXwDHM7-G1Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lWGhlNp5QG9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}